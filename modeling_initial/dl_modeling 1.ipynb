{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, pickle, warnings\n",
    "from pathlib import Path\n",
    "import os, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, precision_recall_fscore_support,\n",
    "                             mean_absolute_error, mean_squared_error, r2_score, precision_score, recall_score,f1_score)\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH               = 64\n",
    "PATIENCE            = 15\n",
    "MIN_ROWS_PER_CLASS  = 3      \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PATH = Path(\"./data/processed_csv.csv\")\n",
    "assert DATA_PATH.exists(), \"processed_csv.csv not found!\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "label_col = \"sg_encoded\" if \"sg_encoded\" in df.columns else \"structure_label\"\n",
    "need = {\"a\",\"b\",\"c\",\"alpha\",\"beta\",\"gamma\",\"vol\",label_col}\n",
    "assert need.issubset(df.columns), f\"CSV must contain {need}\"\n",
    "\n",
    "def add_features(d):\n",
    "    d = d.copy()\n",
    "    for ang in [\"alpha\",\"beta\",\"gamma\"]:\n",
    "        d[f\"cos_{ang}\"] = np.cos(np.deg2rad(d[ang]))\n",
    "    d[\"abc_prod\"] = d[\"a\"]*d[\"b\"]*d[\"c\"]\n",
    "    return d\n",
    "\n",
    "df = add_features(df)\n",
    "NUMERIC = [\"a\",\"b\",\"c\",\"alpha\",\"beta\",\"gamma\",\n",
    "           \"cos_alpha\",\"cos_beta\",\"cos_gamma\",\"abc_prod\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_le = LabelEncoder().fit(df[label_col])\n",
    "counts = np.bincount(tmp_le.transform(df[label_col]))\n",
    "keep_mask = counts[tmp_le.transform(df[label_col])] >= MIN_ROWS_PER_CLASS\n",
    "df = df.loc[keep_mask].reset_index(drop=True)\n",
    "\n",
    "le = LabelEncoder().fit(df[label_col])\n",
    "y_cls = le.transform(df[label_col]).astype(np.int64)\n",
    "y_reg = df[\"vol\"].astype(np.float32).values\n",
    "X     = df[NUMERIC].astype(np.float32).values\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_c, X_tmp_c, y_tr_c, y_tmp_c = train_test_split(\n",
    "    X, y_cls, test_size=0.2, random_state=SEED, stratify=y_cls)\n",
    "\n",
    "X_val_c, X_te_c, y_val_c, y_te_c = train_test_split(\n",
    "    X_tmp_c, y_tmp_c, test_size=0.5, random_state=SEED)  \n",
    "\n",
    "X_tr_r, X_tmp_r, y_tr_r, y_tmp_r = train_test_split(\n",
    "    X, y_reg, test_size=0.2, random_state=SEED)\n",
    "X_val_r, X_te_r, y_val_r, y_te_r = train_test_split(\n",
    "    X_tmp_r, y_tmp_r, test_size=0.5, random_state=SEED)\n",
    "\n",
    "class TabDS(Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        self.X = torch.tensor(X,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self,i): return self.X[i], self.y[i]\n",
    "\n",
    "dls = {\n",
    "    \"train_cls\": DataLoader(TabDS(X_tr_c,y_tr_c),BATCH,shuffle=True),\n",
    "    \"val_cls\"  : DataLoader(TabDS(X_val_c,y_val_c),BATCH),\n",
    "    \"test_cls\" : DataLoader(TabDS(X_te_c ,y_te_c ),BATCH),\n",
    "    \"train_reg\": DataLoader(TabDS(X_tr_r,y_tr_r),BATCH,shuffle=True),\n",
    "    \"val_reg\"  : DataLoader(TabDS(X_val_r,y_val_r),BATCH),\n",
    "    \"test_reg\" : DataLoader(TabDS(X_te_r ,y_te_r ),BATCH),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPCls(nn.Module):\n",
    "    def __init__(self,inp,nc):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(inp,64), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(64,32), nn.ReLU(),\n",
    "            nn.Linear(32,nc)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class MLPReg(nn.Module):\n",
    "    def __init__(self,inp):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(inp,64), nn.ReLU(),\n",
    "            nn.Linear(64,32), nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x).squeeze(1)\n",
    "\n",
    "model_cls = MLPCls(len(NUMERIC), len(le.classes_)).to(DEVICE)\n",
    "model_reg = MLPReg(len(NUMERIC)).to(DEVICE)\n",
    "\n",
    "def train(model, loaders, loss_fn, opt, epochs=200):\n",
    "    hist = {\"tr\":[],\"val\":[]}\n",
    "    best, wait, best_sd = float(\"inf\"),0,None\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train(); tloss=[]\n",
    "        for xb,yb in loaders[\"train\"]:\n",
    "            xb,yb=xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward(); opt.step()\n",
    "            tloss.append(loss.item())\n",
    "        hist[\"tr\"].append(np.mean(tloss))\n",
    "        model.eval(); vloss=[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in loaders[\"val\"]:\n",
    "                xb,yb=xb.to(DEVICE), yb.to(DEVICE)\n",
    "                vloss.append(loss_fn(model(xb), yb).item())\n",
    "        v = np.mean(vloss); hist[\"val\"].append(v)\n",
    "        print(f\"Epoch {ep:03d} | Val Loss = {v:.4f}\")\n",
    "\n",
    "        if v < best - 1e-4:\n",
    "            best, wait = v, 0\n",
    "            best_sd = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= PATIENCE:\n",
    "                print(\"Early stopping\"); break\n",
    "    model.load_state_dict(best_sd)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training classifier ---\n",
      "Epoch 001 | Val Loss = 2.1446\n",
      "Epoch 002 | Val Loss = 1.8782\n",
      "Epoch 003 | Val Loss = 1.7457\n",
      "Epoch 004 | Val Loss = 1.6541\n",
      "Epoch 005 | Val Loss = 1.5897\n",
      "Epoch 006 | Val Loss = 1.5470\n",
      "Epoch 007 | Val Loss = 1.5152\n",
      "Epoch 008 | Val Loss = 1.4883\n",
      "Epoch 009 | Val Loss = 1.4641\n",
      "Epoch 010 | Val Loss = 1.4464\n",
      "Epoch 011 | Val Loss = 1.4297\n",
      "Epoch 012 | Val Loss = 1.4148\n",
      "Epoch 013 | Val Loss = 1.4131\n",
      "Epoch 014 | Val Loss = 1.3992\n",
      "Epoch 015 | Val Loss = 1.3866\n",
      "Epoch 016 | Val Loss = 1.3881\n",
      "Epoch 017 | Val Loss = 1.3784\n",
      "Epoch 018 | Val Loss = 1.3729\n",
      "Epoch 019 | Val Loss = 1.3740\n",
      "Epoch 020 | Val Loss = 1.3600\n",
      "Epoch 021 | Val Loss = 1.3560\n",
      "Epoch 022 | Val Loss = 1.3525\n",
      "Epoch 023 | Val Loss = 1.3424\n",
      "Epoch 024 | Val Loss = 1.3404\n",
      "Epoch 025 | Val Loss = 1.3413\n",
      "Epoch 026 | Val Loss = 1.3373\n",
      "Epoch 027 | Val Loss = 1.3268\n",
      "Epoch 028 | Val Loss = 1.3262\n",
      "Epoch 029 | Val Loss = 1.3253\n",
      "Epoch 030 | Val Loss = 1.3316\n",
      "Epoch 031 | Val Loss = 1.3161\n",
      "Epoch 032 | Val Loss = 1.3223\n",
      "Epoch 033 | Val Loss = 1.3176\n",
      "Epoch 034 | Val Loss = 1.3170\n",
      "Epoch 035 | Val Loss = 1.3141\n",
      "Epoch 036 | Val Loss = 1.3073\n",
      "Epoch 037 | Val Loss = 1.3020\n",
      "Epoch 038 | Val Loss = 1.3028\n",
      "Epoch 039 | Val Loss = 1.3056\n",
      "Epoch 040 | Val Loss = 1.3088\n",
      "Epoch 041 | Val Loss = 1.3021\n",
      "Epoch 042 | Val Loss = 1.3019\n",
      "Epoch 043 | Val Loss = 1.3070\n",
      "Epoch 044 | Val Loss = 1.2911\n",
      "Epoch 045 | Val Loss = 1.2965\n",
      "Epoch 046 | Val Loss = 1.2923\n",
      "Epoch 047 | Val Loss = 1.2875\n",
      "Epoch 048 | Val Loss = 1.2864\n",
      "Epoch 049 | Val Loss = 1.2828\n",
      "Epoch 050 | Val Loss = 1.2842\n",
      "Epoch 051 | Val Loss = 1.2894\n",
      "Epoch 052 | Val Loss = 1.2839\n",
      "Epoch 053 | Val Loss = 1.2806\n",
      "Epoch 054 | Val Loss = 1.2945\n",
      "Epoch 055 | Val Loss = 1.2791\n",
      "Epoch 056 | Val Loss = 1.2749\n",
      "Epoch 057 | Val Loss = 1.2737\n",
      "Epoch 058 | Val Loss = 1.2769\n",
      "Epoch 059 | Val Loss = 1.2762\n",
      "Epoch 060 | Val Loss = 1.2659\n",
      "Epoch 061 | Val Loss = 1.2689\n",
      "Epoch 062 | Val Loss = 1.2737\n",
      "Epoch 063 | Val Loss = 1.2663\n",
      "Epoch 064 | Val Loss = 1.2689\n",
      "Epoch 065 | Val Loss = 1.2675\n",
      "Epoch 066 | Val Loss = 1.2613\n",
      "Epoch 067 | Val Loss = 1.2643\n",
      "Epoch 068 | Val Loss = 1.2611\n",
      "Epoch 069 | Val Loss = 1.2588\n",
      "Epoch 070 | Val Loss = 1.2670\n",
      "Epoch 071 | Val Loss = 1.2614\n",
      "Epoch 072 | Val Loss = 1.2628\n",
      "Epoch 073 | Val Loss = 1.2600\n",
      "Epoch 074 | Val Loss = 1.2670\n",
      "Epoch 075 | Val Loss = 1.2577\n",
      "Epoch 076 | Val Loss = 1.2589\n",
      "Epoch 077 | Val Loss = 1.2569\n",
      "Epoch 078 | Val Loss = 1.2598\n",
      "Epoch 079 | Val Loss = 1.2573\n",
      "Epoch 080 | Val Loss = 1.2521\n",
      "Epoch 081 | Val Loss = 1.2480\n",
      "Epoch 082 | Val Loss = 1.2558\n",
      "Epoch 083 | Val Loss = 1.2539\n",
      "Epoch 084 | Val Loss = 1.2671\n",
      "Epoch 085 | Val Loss = 1.2480\n",
      "Epoch 086 | Val Loss = 1.2523\n",
      "Epoch 087 | Val Loss = 1.2521\n",
      "Epoch 088 | Val Loss = 1.2463\n",
      "Epoch 089 | Val Loss = 1.2467\n",
      "Epoch 090 | Val Loss = 1.2608\n",
      "Epoch 091 | Val Loss = 1.2524\n",
      "Epoch 092 | Val Loss = 1.2516\n",
      "Epoch 093 | Val Loss = 1.2626\n",
      "Epoch 094 | Val Loss = 1.2456\n",
      "Epoch 095 | Val Loss = 1.2483\n",
      "Epoch 096 | Val Loss = 1.2470\n",
      "Epoch 097 | Val Loss = 1.2529\n",
      "Epoch 098 | Val Loss = 1.2439\n",
      "Epoch 099 | Val Loss = 1.2434\n",
      "Epoch 100 | Val Loss = 1.2435\n",
      "Epoch 101 | Val Loss = 1.2431\n",
      "Epoch 102 | Val Loss = 1.2385\n",
      "Epoch 103 | Val Loss = 1.2433\n",
      "Epoch 104 | Val Loss = 1.2360\n",
      "Epoch 105 | Val Loss = 1.2367\n",
      "Epoch 106 | Val Loss = 1.2379\n",
      "Epoch 107 | Val Loss = 1.2315\n",
      "Epoch 108 | Val Loss = 1.2448\n",
      "Epoch 109 | Val Loss = 1.2353\n",
      "Epoch 110 | Val Loss = 1.2349\n",
      "Epoch 111 | Val Loss = 1.2293\n",
      "Epoch 112 | Val Loss = 1.2402\n",
      "Epoch 113 | Val Loss = 1.2414\n",
      "Epoch 114 | Val Loss = 1.2366\n",
      "Epoch 115 | Val Loss = 1.2320\n",
      "Epoch 116 | Val Loss = 1.2388\n",
      "Epoch 117 | Val Loss = 1.2423\n",
      "Epoch 118 | Val Loss = 1.2330\n",
      "Epoch 119 | Val Loss = 1.2379\n",
      "Epoch 120 | Val Loss = 1.2389\n",
      "Epoch 121 | Val Loss = 1.2366\n",
      "Epoch 122 | Val Loss = 1.2327\n",
      "Epoch 123 | Val Loss = 1.2277\n",
      "Epoch 124 | Val Loss = 1.2309\n",
      "Epoch 125 | Val Loss = 1.2318\n",
      "Epoch 126 | Val Loss = 1.2334\n",
      "Epoch 127 | Val Loss = 1.2391\n",
      "Epoch 128 | Val Loss = 1.2364\n",
      "Epoch 129 | Val Loss = 1.2291\n",
      "Epoch 130 | Val Loss = 1.2320\n",
      "Epoch 131 | Val Loss = 1.2321\n",
      "Epoch 132 | Val Loss = 1.2402\n",
      "Epoch 133 | Val Loss = 1.2352\n",
      "Epoch 134 | Val Loss = 1.2408\n",
      "Epoch 135 | Val Loss = 1.2321\n",
      "Epoch 136 | Val Loss = 1.2382\n",
      "Epoch 137 | Val Loss = 1.2418\n",
      "Epoch 138 | Val Loss = 1.2424\n",
      "Early stopping\n",
      "\n",
      "--- Training regressor ---\n",
      "Epoch 001 | Val Loss = 0.9910\n",
      "Epoch 002 | Val Loss = 0.3848\n",
      "Epoch 003 | Val Loss = 0.0137\n",
      "Epoch 004 | Val Loss = 0.0162\n",
      "Epoch 005 | Val Loss = 0.0165\n",
      "Epoch 006 | Val Loss = 0.0185\n",
      "Epoch 007 | Val Loss = 0.0183\n",
      "Epoch 008 | Val Loss = 0.0167\n",
      "Epoch 009 | Val Loss = 0.0147\n",
      "Epoch 010 | Val Loss = 0.0124\n",
      "Epoch 011 | Val Loss = 0.0164\n",
      "Epoch 012 | Val Loss = 0.0115\n",
      "Epoch 013 | Val Loss = 0.0148\n",
      "Epoch 014 | Val Loss = 0.0246\n",
      "Epoch 015 | Val Loss = 0.0154\n",
      "Epoch 016 | Val Loss = 0.0157\n",
      "Epoch 017 | Val Loss = 0.0086\n",
      "Epoch 018 | Val Loss = 0.0666\n",
      "Epoch 019 | Val Loss = 0.0567\n",
      "Epoch 020 | Val Loss = 0.0070\n",
      "Epoch 021 | Val Loss = 0.0156\n",
      "Epoch 022 | Val Loss = 0.0137\n",
      "Epoch 023 | Val Loss = 0.0170\n",
      "Epoch 024 | Val Loss = 0.0066\n",
      "Epoch 025 | Val Loss = 0.0458\n",
      "Epoch 026 | Val Loss = 0.0095\n",
      "Epoch 027 | Val Loss = 0.0825\n",
      "Epoch 028 | Val Loss = 0.0069\n",
      "Epoch 029 | Val Loss = 0.0224\n",
      "Epoch 030 | Val Loss = 0.0128\n",
      "Epoch 031 | Val Loss = 0.0243\n",
      "Epoch 032 | Val Loss = 0.0085\n",
      "Epoch 033 | Val Loss = 0.0417\n",
      "Epoch 034 | Val Loss = 0.0106\n",
      "Epoch 035 | Val Loss = 0.0918\n",
      "Epoch 036 | Val Loss = 0.0137\n",
      "Epoch 037 | Val Loss = 0.0177\n",
      "Epoch 038 | Val Loss = 0.0195\n",
      "Epoch 039 | Val Loss = 0.0154\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training classifier ---\")\n",
    "hist_cls = train(model_cls,\n",
    "                 {\"train\":dls[\"train_cls\"],\"val\":dls[\"val_cls\"]},\n",
    "                 nn.CrossEntropyLoss(),\n",
    "                 torch.optim.Adam(model_cls.parameters(),lr=1e-3))\n",
    "\n",
    "print(\"\\n--- Training regressor ---\")\n",
    "hist_reg = train(model_reg,\n",
    "                 {\"train\":dls[\"train_reg\"],\"val\":dls[\"val_reg\"]},\n",
    "                 nn.MSELoss(),\n",
    "                 torch.optim.Adam(model_reg.parameters(),lr=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CLASSIFICATION RESULTS ===\n",
      "Test accuracy : 0.5979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000         2\n",
      "           3      0.000     0.000     0.000         6\n",
      "           4      0.573     0.580     0.577        81\n",
      "           5      0.000     0.000     0.000         8\n",
      "           6      0.000     0.000     0.000         9\n",
      "           9      0.000     0.000     0.000         2\n",
      "          13      0.000     0.000     0.000         1\n",
      "          14      0.000     0.000     0.000         1\n",
      "          15      0.000     0.000     0.000         1\n",
      "          16      0.000     0.000     0.000         1\n",
      "          22      0.333     1.000     0.500         1\n",
      "          23      1.000     1.000     1.000         2\n",
      "          24      0.667     0.667     0.667         3\n",
      "          27      0.500     1.000     0.667         3\n",
      "          28      0.000     0.000     0.000         1\n",
      "          29      0.000     0.000     0.000         1\n",
      "          35      0.000     0.000     0.000         2\n",
      "          36      0.000     0.000     0.000        15\n",
      "          38      0.000     0.000     0.000         2\n",
      "          39      0.000     0.000     0.000         1\n",
      "          45      0.000     0.000     0.000         1\n",
      "          46      0.333     0.333     0.333         3\n",
      "          50      1.000     1.000     1.000         1\n",
      "          53      0.200     0.500     0.286         4\n",
      "          54      1.000     0.500     0.667         2\n",
      "          56      0.000     0.000     0.000         1\n",
      "          60      0.000     0.000     0.000         1\n",
      "          61      0.000     0.000     0.000         1\n",
      "          66      0.974     0.984     0.979       304\n",
      "          67      0.000     0.000     0.000         3\n",
      "          68      0.000     0.000     0.000         1\n",
      "          71      0.000     0.000     0.000         1\n",
      "          74      0.000     0.000     0.000         4\n",
      "          81      0.000     0.000     0.000         1\n",
      "          82      0.000     0.000     0.000         1\n",
      "          84      0.000     0.000     0.000         1\n",
      "          85      0.000     0.000     0.000         9\n",
      "          87      0.000     0.000     0.000         1\n",
      "          88      0.000     0.000     0.000         5\n",
      "          90      0.667     0.667     0.667         3\n",
      "          91      0.344     0.200     0.253        55\n",
      "          93      0.531     0.371     0.437       210\n",
      "          94      0.000     0.000     0.000         6\n",
      "          95      0.422     0.772     0.546       162\n",
      "          96      0.000     0.000     0.000         4\n",
      "          98      0.000     0.000     0.000         3\n",
      "         100      0.250     0.400     0.308         5\n",
      "         101      0.564     0.846     0.677        52\n",
      "         102      0.333     1.000     0.500         2\n",
      "         107      0.000     0.000     0.000         1\n",
      "         109      0.000     0.000     0.000         1\n",
      "         114      0.000     0.000     0.000         1\n",
      "         117      0.000     0.000     0.000         1\n",
      "         118      0.000     0.000     0.000         1\n",
      "         120      0.000     0.000     0.000         4\n",
      "         125      0.000     0.000     0.000         1\n",
      "         129      0.000     0.000     0.000         2\n",
      "         131      1.000     1.000     1.000         1\n",
      "         135      0.000     0.000     0.000         2\n",
      "         136      0.000     0.000     0.000         1\n",
      "         138      0.000     0.000     0.000         1\n",
      "         139      1.000     1.000     1.000         2\n",
      "         141      0.500     1.000     0.667         1\n",
      "         143      1.000     1.000     1.000         1\n",
      "         145      0.000     0.000     0.000         1\n",
      "         149      0.340     0.533     0.416        30\n",
      "         151      0.000     0.000     0.000         1\n",
      "         152      0.000     0.000     0.000        13\n",
      "         154      0.000     0.000     0.000        16\n",
      "         158      0.000     0.000     0.000         2\n",
      "         164      0.000     0.000     0.000         1\n",
      "         169      0.250     0.267     0.258        15\n",
      "         172      0.387     0.800     0.522        15\n",
      "         174      0.000     0.000     0.000         1\n",
      "         175      0.000     0.000     0.000         2\n",
      "         177      0.391     0.900     0.545        10\n",
      "         178      0.500     1.000     0.667         1\n",
      "         179      0.500     0.667     0.571         3\n",
      "         181      0.000     0.000     0.000         1\n",
      "         182      0.000     0.000     0.000         1\n",
      "\n",
      "   micro avg      0.599     0.598     0.598      1124\n",
      "   macro avg      0.195     0.250     0.209      1124\n",
      "weighted avg      0.547     0.598     0.557      1124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cls.eval(); yp,yt=[],[]\n",
    "with torch.no_grad():\n",
    "    for xb,yb in dls[\"test_cls\"]:\n",
    "        yp.append(torch.argmax(model_cls(xb.to(DEVICE)),1).cpu().numpy())\n",
    "        yt.append(yb.numpy())\n",
    "y_pred_cls = np.concatenate(yp); y_true_cls=np.concatenate(yt)\n",
    "acc = accuracy_score(y_true_cls,y_pred_cls)\n",
    "\n",
    "labels_used = np.sort(np.unique(y_true_cls))\n",
    "names_used  = [str(le.inverse_transform([i])[0]) for i in labels_used]\n",
    "\n",
    "print(\"\\n=== CLASSIFICATION RESULTS ===\")\n",
    "print(f\"Test accuracy : {acc:.4f}\")\n",
    "print(classification_report(y_true_cls,y_pred_cls,\n",
    "                            labels=labels_used,\n",
    "                            target_names=names_used, digits=3))\n",
    "cm = confusion_matrix(y_true_cls,y_pred_cls,labels=labels_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REGRESSION RESULTS ===\n",
      "RMSE: 0.061\n",
      "MAE : 0.015\n",
      "R²  : 0.9973\n"
     ]
    }
   ],
   "source": [
    "model_reg.eval(); yp,yt=[],[]\n",
    "with torch.no_grad():\n",
    "    for xb,yb in dls[\"test_reg\"]:\n",
    "        yp.append(model_reg(xb.to(DEVICE)).cpu().numpy())\n",
    "        yt.append(yb.numpy())\n",
    "y_pred_reg = np.concatenate(yp); y_true_reg = np.concatenate(yt)\n",
    "rmse = math.sqrt(mean_squared_error(y_true_reg,y_pred_reg))\n",
    "mae  = mean_absolute_error(y_true_reg,y_pred_reg)\n",
    "r2   = r2_score(y_true_reg,y_pred_reg)\n",
    "\n",
    "print(\"\\n=== REGRESSION RESULTS ===\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAE : {mae :.3f}\")\n",
    "print(f\"R²  : {r2  :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(hist, title):\n",
    "    epochs = range(1, len(hist[\"tr\"]) + 1)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(epochs, hist[\"tr\"], label=\"Train\")\n",
    "    plt.plot(epochs, hist[\"val\"], label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(hist_cls, \"Classifier – Cross‑Entropy Loss\")\n",
    "\n",
    "# Confusion matrix heat-map\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error skew bar – false-negative vs false-positive balance\n",
    "fp_mask  = (y_pred_cls != y_true_cls) & np.isin(y_pred_cls, labels_used)\n",
    "fn_mask  = (y_pred_cls != y_true_cls) & np.isin(y_true_cls, labels_used)\n",
    "\n",
    "fp_count = pd.Series(y_pred_cls[fp_mask]).value_counts()\n",
    "fn_count = pd.Series(y_true_cls[fn_mask]).value_counts()\n",
    "\n",
    "err_df = pd.DataFrame({\"FP\": fp_count, \"FN\": fn_count}).reindex(labels_used, fill_value=0)\n",
    "\n",
    "threshold = 15\n",
    "sig_err_df = err_df[(err_df[\"FP\"] + err_df[\"FN\"]) >= threshold]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sig_err_df.plot(kind=\"bar\", width=0.8, color={\"FP\":\"tab:red\", \"FN\":\"tab:blue\"}, ax=plt.gca())\n",
    "\n",
    "plt.title(f\"False Positives vs False Negatives (combined errors ≥ {threshold})\")\n",
    "plt.xlabel(\"Class ID\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Error type\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plot_loss(hist_reg, \"Regressor – MSE Loss\")\n",
    "\n",
    "# True vs Predicted scatter\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(y_true_reg, y_pred_reg, s=15, alpha=0.4)\n",
    "lims = [min(y_true_reg.min(), y_pred_reg.min()), max(y_true_reg.max(), y_pred_reg.max())]\n",
    "plt.plot(lims, lims, color='red', linestyle='--', linewidth=1)\n",
    "plt.xlim(0, 8)\n",
    "plt.ylim(0, 8)\n",
    "plt.xlabel(\"True Volume\")\n",
    "plt.ylabel(\"Predicted Volume\")\n",
    "plt.title(\"Volume – True vs Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'Models' created successfully.\n",
      "Directory 'Models\\Model 1' created successfully.\n",
      "\n",
      "Saved files to 'Models\\Model 1':\n",
      "  - Model 1 spacegroup.pt\n",
      "  - Model 1 volume_regressor.pt\n",
      "  - Model 1 preprocessing.pkl\n"
     ]
    }
   ],
   "source": [
    "parent_dir    = \"Models\"\n",
    "model_name    = \"Model 1\"\n",
    "model_dir     = os.path.join(parent_dir, model_name)\n",
    "\n",
    "try:\n",
    "    os.mkdir(parent_dir)\n",
    "    print(f\"Directory '{parent_dir}' created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{parent_dir}' already exists.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied: Unable to create '{parent_dir}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(model_dir)\n",
    "    print(f\"Directory '{model_dir}' created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{model_dir}' already exists.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied: Unable to create '{model_dir}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "torch.save(\n",
    "    model_cls.state_dict(),\n",
    "    os.path.join(model_dir, f\"{model_name} spacegroup.pt\")\n",
    ")\n",
    "torch.save(\n",
    "    model_reg.state_dict(),\n",
    "    os.path.join(model_dir, f\"{model_name} volume_regressor.pt\")\n",
    ")\n",
    "with open(os.path.join(model_dir, f\"{model_name} preprocessing.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\"scaler\": scaler, \"label_encoder\": le},\n",
    "        f\n",
    "    )\n",
    "\n",
    "print(f\"\\nSaved files to '{model_dir}':\")\n",
    "print(f\"  - {model_name} spacegroup.pt\")\n",
    "print(f\"  - {model_name} volume_regressor.pt\")\n",
    "print(f\"  - {model_name} preprocessing.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
